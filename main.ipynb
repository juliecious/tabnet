{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torchvision.utils as vutils\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "import os\n",
    "import wget\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading file...\n"
     ]
    }
   ],
   "source": [
    "# Download census-income dataset\n",
    "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data\"\n",
    "dataset_name = 'census-income'\n",
    "out = Path(os.getcwd()+'/data/'+dataset_name+'.csv')\n",
    "\n",
    "out.parent.mkdir(parents=True, exist_ok=True)\n",
    "if out.exists():\n",
    "    print(\"File already exists.\")\n",
    "else:\n",
    "    print(\"Downloading file...\")\n",
    "    wget.download(url, out.as_posix())\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "# Load data and split\n",
    "train = pd.read_csv(out)\n",
    "target = ' <=50K'\n",
    "if \"Set\" not in train.columns:\n",
    "    train[\"Set\"] = np.random.choice([\"train\", \"valid\", \"test\"], p =[.8, .1, .1], size=(train.shape[0],))\n",
    "\n",
    "train_indices = train[train.Set==\"train\"].index\n",
    "valid_indices = train[train.Set==\"valid\"].index\n",
    "test_indices = train[train.Set==\"test\"].index"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39 73\n",
      " State-gov 9\n",
      " Bachelors 16\n",
      " 13 16\n",
      " Never-married 7\n",
      " Adm-clerical 15\n",
      " Not-in-family 6\n",
      " White 5\n",
      " Male 2\n",
      " 2174 119\n",
      " 0 92\n",
      " 40 94\n",
      " United-States 42\n",
      " <=50K 2\n",
      "Set 3\n"
     ]
    }
   ],
   "source": [
    "# Label encode categorical features and fill empty cells.\n",
    "nunique = train.nunique()\n",
    "types = train.dtypes\n",
    "\n",
    "categorical_columns = []\n",
    "categorical_dims =  {}\n",
    "for col in train.columns:\n",
    "    if types[col] == 'object' or nunique[col] < 200:\n",
    "        print(col, train[col].nunique())\n",
    "        l_enc = LabelEncoder()\n",
    "        train[col] = train[col].fillna(\"VV_likely\")\n",
    "        train[col] = l_enc.fit_transform(train[col].values)\n",
    "        categorical_columns.append(col)\n",
    "        categorical_dims[col] = len(l_enc.classes_)\n",
    "    else:\n",
    "        train.fillna(train.loc[train_indices, col].mean(), inplace=True)\n",
    "\n",
    "# check that pipeline accepts strings\n",
    "train.loc[train[target]==0, target] = \"wealthy\"\n",
    "train.loc[train[target]==1, target] = \"not_wealthy\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "# Define categorical features for categorical embeddings\n",
    "\n",
    "unused_feat = ['Set']\n",
    "features = [ col for col in train.columns if col not in unused_feat+[target]]\n",
    "cat_idxs = [ i for i, f in enumerate(features) if f in categorical_columns]\n",
    "cat_dims = [ categorical_dims[f] for i, f in enumerate(features) if f in categorical_columns]\n",
    "cat_emb_dim=3\n",
    "optimizer_fn=torch.optim.Adam\n",
    "optimizer_params=dict(lr=2e-2)\n",
    "scheduler_params={\"step_size\":50, # how to use learning rate scheduler\n",
    "                  \"gamma\":0.9}\n",
    "scheduler_fn=torch.optim.lr_scheduler.StepLR\n",
    "n_shared_decoder=1 # nb shared glu for decoding\n",
    "n_indep_decoder=1 # nb independent glu for decoding"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "X_train = train[features].values[train_indices]\n",
    "y_train = train[target].values[train_indices]\n",
    "\n",
    "X_valid = train[features].values[valid_indices]\n",
    "y_valid = train[target].values[valid_indices]\n",
    "\n",
    "X_test = train[features].values[test_indices]\n",
    "y_test = train[target].values[test_indices]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device used : cpu\n"
     ]
    }
   ],
   "source": [
    "from pytorch_tabnet.pretraining import TabNetPretrainer\n",
    "\n",
    "# TabNetPretrainer\n",
    "unsupervised_model = TabNetPretrainer(\n",
    "    cat_idxs=cat_idxs,\n",
    "    cat_dims=cat_dims,\n",
    "    cat_emb_dim=3,\n",
    "    optimizer_fn=torch.optim.Adam,\n",
    "    optimizer_params=dict(lr=2e-2),\n",
    "    mask_type='entmax', # \"sparsemax\",\n",
    "    n_shared_decoder=1, # nb shared glu for decoding\n",
    "    n_indep_decoder=1, # nb independent glu for decoding\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "data": {
      "text/plain": "1"
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_epochs = 1 if not os.getenv(\"CI\", False) else 2\n",
    "max_epochs"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 6.57988 | val_0_unsup_loss: 1.9971  |  0:00:09s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_unsup_loss = 1.9971\n",
      "Best weights from best epoch are automatically used!\n",
      "epoch 0  | loss: 1.60911 | val_0_unsup_loss: 1.22277 |  0:00:06s\n",
      "Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_unsup_loss = 1.22277\n",
      "Best weights from best epoch are automatically used!\n"
     ]
    }
   ],
   "source": [
    "\"\"\" ==================== GENERATOR ======================== \"\"\"\n",
    "\n",
    "unsupervised_model.fit(\n",
    "    X_train=X_train,\n",
    "    eval_set=[X_valid],\n",
    "    max_epochs=max_epochs , patience=5,\n",
    "    batch_size=2048, virtual_batch_size=128,\n",
    "    num_workers=0,\n",
    "    drop_last=False,\n",
    "    pretraining_ratio=0.8,\n",
    "\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "data": {
      "text/plain": "TabNetPretraining(\n  (embedder): EmbeddingGenerator(\n    (embeddings): ModuleList(\n      (0): Embedding(73, 3)\n      (1): Embedding(9, 3)\n      (2): Embedding(16, 3)\n      (3): Embedding(16, 3)\n      (4): Embedding(7, 3)\n      (5): Embedding(15, 3)\n      (6): Embedding(6, 3)\n      (7): Embedding(5, 3)\n      (8): Embedding(2, 3)\n      (9): Embedding(119, 3)\n      (10): Embedding(92, 3)\n      (11): Embedding(94, 3)\n      (12): Embedding(42, 3)\n    )\n  )\n  (masker): RandomObfuscator()\n  (encoder): TabNetEncoder(\n    (initial_bn): BatchNorm1d(40, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)\n    (initial_splitter): FeatTransformer(\n      (shared): GLU_Block(\n        (shared_layers): ModuleList(\n          (0): Linear(in_features=40, out_features=32, bias=False)\n          (1): Linear(in_features=16, out_features=32, bias=False)\n        )\n        (glu_layers): ModuleList(\n          (0): GLU_Layer(\n            (fc): Linear(in_features=40, out_features=32, bias=False)\n            (bn): GBN(\n              (bn): BatchNorm1d(32, eps=1e-05, momentum=0.02, affine=True, track_running_stats=True)\n            )\n          )\n          (1): GLU_Layer(\n            (fc): Linear(in_features=16, out_features=32, bias=False)\n            (bn): GBN(\n              (bn): BatchNorm1d(32, eps=1e-05, momentum=0.02, affine=True, track_running_stats=True)\n            )\n          )\n        )\n      )\n      (specifics): GLU_Block(\n        (glu_layers): ModuleList(\n          (0): GLU_Layer(\n            (fc): Linear(in_features=16, out_features=32, bias=False)\n            (bn): GBN(\n              (bn): BatchNorm1d(32, eps=1e-05, momentum=0.02, affine=True, track_running_stats=True)\n            )\n          )\n          (1): GLU_Layer(\n            (fc): Linear(in_features=16, out_features=32, bias=False)\n            (bn): GBN(\n              (bn): BatchNorm1d(32, eps=1e-05, momentum=0.02, affine=True, track_running_stats=True)\n            )\n          )\n        )\n      )\n    )\n    (feat_transformers): ModuleList(\n      (0): FeatTransformer(\n        (shared): GLU_Block(\n          (shared_layers): ModuleList(\n            (0): Linear(in_features=40, out_features=32, bias=False)\n            (1): Linear(in_features=16, out_features=32, bias=False)\n          )\n          (glu_layers): ModuleList(\n            (0): GLU_Layer(\n              (fc): Linear(in_features=40, out_features=32, bias=False)\n              (bn): GBN(\n                (bn): BatchNorm1d(32, eps=1e-05, momentum=0.02, affine=True, track_running_stats=True)\n              )\n            )\n            (1): GLU_Layer(\n              (fc): Linear(in_features=16, out_features=32, bias=False)\n              (bn): GBN(\n                (bn): BatchNorm1d(32, eps=1e-05, momentum=0.02, affine=True, track_running_stats=True)\n              )\n            )\n          )\n        )\n        (specifics): GLU_Block(\n          (glu_layers): ModuleList(\n            (0): GLU_Layer(\n              (fc): Linear(in_features=16, out_features=32, bias=False)\n              (bn): GBN(\n                (bn): BatchNorm1d(32, eps=1e-05, momentum=0.02, affine=True, track_running_stats=True)\n              )\n            )\n            (1): GLU_Layer(\n              (fc): Linear(in_features=16, out_features=32, bias=False)\n              (bn): GBN(\n                (bn): BatchNorm1d(32, eps=1e-05, momentum=0.02, affine=True, track_running_stats=True)\n              )\n            )\n          )\n        )\n      )\n      (1): FeatTransformer(\n        (shared): GLU_Block(\n          (shared_layers): ModuleList(\n            (0): Linear(in_features=40, out_features=32, bias=False)\n            (1): Linear(in_features=16, out_features=32, bias=False)\n          )\n          (glu_layers): ModuleList(\n            (0): GLU_Layer(\n              (fc): Linear(in_features=40, out_features=32, bias=False)\n              (bn): GBN(\n                (bn): BatchNorm1d(32, eps=1e-05, momentum=0.02, affine=True, track_running_stats=True)\n              )\n            )\n            (1): GLU_Layer(\n              (fc): Linear(in_features=16, out_features=32, bias=False)\n              (bn): GBN(\n                (bn): BatchNorm1d(32, eps=1e-05, momentum=0.02, affine=True, track_running_stats=True)\n              )\n            )\n          )\n        )\n        (specifics): GLU_Block(\n          (glu_layers): ModuleList(\n            (0): GLU_Layer(\n              (fc): Linear(in_features=16, out_features=32, bias=False)\n              (bn): GBN(\n                (bn): BatchNorm1d(32, eps=1e-05, momentum=0.02, affine=True, track_running_stats=True)\n              )\n            )\n            (1): GLU_Layer(\n              (fc): Linear(in_features=16, out_features=32, bias=False)\n              (bn): GBN(\n                (bn): BatchNorm1d(32, eps=1e-05, momentum=0.02, affine=True, track_running_stats=True)\n              )\n            )\n          )\n        )\n      )\n      (2): FeatTransformer(\n        (shared): GLU_Block(\n          (shared_layers): ModuleList(\n            (0): Linear(in_features=40, out_features=32, bias=False)\n            (1): Linear(in_features=16, out_features=32, bias=False)\n          )\n          (glu_layers): ModuleList(\n            (0): GLU_Layer(\n              (fc): Linear(in_features=40, out_features=32, bias=False)\n              (bn): GBN(\n                (bn): BatchNorm1d(32, eps=1e-05, momentum=0.02, affine=True, track_running_stats=True)\n              )\n            )\n            (1): GLU_Layer(\n              (fc): Linear(in_features=16, out_features=32, bias=False)\n              (bn): GBN(\n                (bn): BatchNorm1d(32, eps=1e-05, momentum=0.02, affine=True, track_running_stats=True)\n              )\n            )\n          )\n        )\n        (specifics): GLU_Block(\n          (glu_layers): ModuleList(\n            (0): GLU_Layer(\n              (fc): Linear(in_features=16, out_features=32, bias=False)\n              (bn): GBN(\n                (bn): BatchNorm1d(32, eps=1e-05, momentum=0.02, affine=True, track_running_stats=True)\n              )\n            )\n            (1): GLU_Layer(\n              (fc): Linear(in_features=16, out_features=32, bias=False)\n              (bn): GBN(\n                (bn): BatchNorm1d(32, eps=1e-05, momentum=0.02, affine=True, track_running_stats=True)\n              )\n            )\n          )\n        )\n      )\n    )\n    (att_transformers): ModuleList(\n      (0): AttentiveTransformer(\n        (fc): Linear(in_features=8, out_features=40, bias=False)\n        (bn): GBN(\n          (bn): BatchNorm1d(40, eps=1e-05, momentum=0.02, affine=True, track_running_stats=True)\n        )\n        (selector): Entmax15()\n      )\n      (1): AttentiveTransformer(\n        (fc): Linear(in_features=8, out_features=40, bias=False)\n        (bn): GBN(\n          (bn): BatchNorm1d(40, eps=1e-05, momentum=0.02, affine=True, track_running_stats=True)\n        )\n        (selector): Entmax15()\n      )\n      (2): AttentiveTransformer(\n        (fc): Linear(in_features=8, out_features=40, bias=False)\n        (bn): GBN(\n          (bn): BatchNorm1d(40, eps=1e-05, momentum=0.02, affine=True, track_running_stats=True)\n        )\n        (selector): Entmax15()\n      )\n    )\n  )\n  (decoder): TabNetDecoder(\n    (feat_transformers): ModuleList(\n      (0): FeatTransformer(\n        (shared): GLU_Block(\n          (shared_layers): ModuleList(\n            (0): Linear(in_features=8, out_features=16, bias=False)\n          )\n          (glu_layers): ModuleList(\n            (0): GLU_Layer(\n              (fc): Linear(in_features=8, out_features=16, bias=False)\n              (bn): GBN(\n                (bn): BatchNorm1d(16, eps=1e-05, momentum=0.02, affine=True, track_running_stats=True)\n              )\n            )\n          )\n        )\n        (specifics): GLU_Block(\n          (glu_layers): ModuleList(\n            (0): GLU_Layer(\n              (fc): Linear(in_features=8, out_features=16, bias=False)\n              (bn): GBN(\n                (bn): BatchNorm1d(16, eps=1e-05, momentum=0.02, affine=True, track_running_stats=True)\n              )\n            )\n          )\n        )\n      )\n      (1): FeatTransformer(\n        (shared): GLU_Block(\n          (shared_layers): ModuleList(\n            (0): Linear(in_features=8, out_features=16, bias=False)\n          )\n          (glu_layers): ModuleList(\n            (0): GLU_Layer(\n              (fc): Linear(in_features=8, out_features=16, bias=False)\n              (bn): GBN(\n                (bn): BatchNorm1d(16, eps=1e-05, momentum=0.02, affine=True, track_running_stats=True)\n              )\n            )\n          )\n        )\n        (specifics): GLU_Block(\n          (glu_layers): ModuleList(\n            (0): GLU_Layer(\n              (fc): Linear(in_features=8, out_features=16, bias=False)\n              (bn): GBN(\n                (bn): BatchNorm1d(16, eps=1e-05, momentum=0.02, affine=True, track_running_stats=True)\n              )\n            )\n          )\n        )\n      )\n      (2): FeatTransformer(\n        (shared): GLU_Block(\n          (shared_layers): ModuleList(\n            (0): Linear(in_features=8, out_features=16, bias=False)\n          )\n          (glu_layers): ModuleList(\n            (0): GLU_Layer(\n              (fc): Linear(in_features=8, out_features=16, bias=False)\n              (bn): GBN(\n                (bn): BatchNorm1d(16, eps=1e-05, momentum=0.02, affine=True, track_running_stats=True)\n              )\n            )\n          )\n        )\n        (specifics): GLU_Block(\n          (glu_layers): ModuleList(\n            (0): GLU_Layer(\n              (fc): Linear(in_features=8, out_features=16, bias=False)\n              (bn): GBN(\n                (bn): BatchNorm1d(16, eps=1e-05, momentum=0.02, affine=True, track_running_stats=True)\n              )\n            )\n          )\n        )\n      )\n    )\n    (reconstruction_layer): Linear(in_features=8, out_features=40, bias=False)\n  )\n)"
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# gerator = encoder + decoder\n",
    "generator = unsupervised_model.network\n",
    "generator"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "# Make reconstruction from a dataset\n",
    "reconstructed_X, embedded_X = unsupervised_model.predict(X_valid)\n",
    "assert(reconstructed_X.shape==embedded_X.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[-0.03961663, -0.24110238,  0.47189063, ..., -0.02406411,\n        -0.95372117,  1.5843254 ],\n       [-0.0527579 , -0.21691859,  0.39570072, ..., -0.02289679,\n        -0.9455302 ,  1.4653947 ],\n       [-0.05943928, -0.18790035,  0.37794593, ...,  0.00494059,\n        -0.8585742 ,  1.4165832 ],\n       ...,\n       [-0.05337216, -0.24408716,  0.5111049 , ..., -0.02050782,\n        -0.9150843 ,  1.575053  ],\n       [-0.01427674, -0.26014954,  0.5003153 , ...,  0.03300002,\n        -0.8445887 ,  1.6179703 ],\n       [-0.07667636, -0.22761166,  0.343219  , ...,  0.00909834,\n        -0.9047952 ,  1.3742868 ]], dtype=float32)"
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reconstructed_X"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# nn.Embedding(10, 10).weight.shape\n",
    "\n",
    "n_classes = 2"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 29,
   "outputs": [
    {
     "data": {
      "text/plain": "ModuleList(\n  (0): Embedding(73, 3)\n  (1): Embedding(9, 3)\n  (2): Embedding(16, 3)\n  (3): Embedding(16, 3)\n  (4): Embedding(7, 3)\n  (5): Embedding(15, 3)\n  (6): Embedding(6, 3)\n  (7): Embedding(5, 3)\n  (8): Embedding(2, 3)\n  (9): Embedding(119, 3)\n  (10): Embedding(92, 3)\n  (11): Embedding(94, 3)\n  (12): Embedding(42, 3)\n)"
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Discrimator\n",
    "\n",
    "\"\"\" ==================== DISCRIMINATOR ======================== \"\"\"\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "\tdef __init__(self):\n",
    "\t\tsuper(Discriminator, self).__init__()\n",
    "\t\tself.label_embed1 = nn.Embedding(n_classes, n_classes)\n",
    "\t\tself.dropout = 0.4\n",
    "\t\tself.depth = 512\n",
    "\n",
    "\t\tdef init(input, output, normalize=True):\n",
    "\t\t\tlayers = [nn.Linear(input, output)]\n",
    "\t\t\tif normalize:\n",
    "\t\t\t\tlayers.append(nn.Dropout(self.dropout))\n",
    "\t\t\tlayers.append(nn.LeakyReLU(0.2, inplace=True))\n",
    "\t\t\treturn layers\n",
    "\n",
    "\t\tself.discriminator = nn.Sequential(\n",
    "\t\t\t*init(n_classes+int(np.prod(img_shape)), self.depth, normalize=False),\n",
    "\t\t\t*init(self.depth, self.depth),\n",
    "\t\t\t*init(self.depth, self.depth),\n",
    "\t\t\tnn.Linear(self.depth, 1),\n",
    "\t\t\tnn.Sigmoid()\n",
    "\t\t\t)\n",
    "\n",
    "\tdef forward(self, img, labels):\n",
    "\t\timgs = img.view(img.size(0),-1)\n",
    "\t\tinpu = torch.cat((imgs, self.label_embed1(labels)), -1)\n",
    "\t\tvalidity = self.discriminator(inpu)\n",
    "\t\treturn validity\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [],
   "source": [
    "# weight initialization\n",
    "def init_weights(m):\n",
    "\tif type(m)==nn.Linear:\n",
    "\t\ttorch.nn.init.xavier_uniform_(m.weight)\n",
    "\t\tm.bias.data.fill_(0.01)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "beta = 0.5\n",
    "beta1 = 0.999\n",
    "lrate = 0.0002\n",
    "\n",
    "# Building generator\n",
    "generator = unsupervised_model.network\n",
    "# gen_optimizer = torch.optim.Adam(generator.parameters(), lr=lrate, betas=(beta, beta1))\n",
    "\n",
    "# Building discriminator\n",
    "discriminator = Discriminator()\n",
    "discriminator.apply(init_weights)\n",
    "d_optimizer = torch.optim.Adam(discriminator.parameters(), lr=lrate, betas=(beta, beta1))\n",
    "\n",
    "# Loss functions\n",
    "a_loss = torch.nn.BCELoss()\n",
    "\n",
    "# Labels\n",
    "real_label = 0.9\n",
    "fake_label = 0.0"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "FT = torch.LongTensor\n",
    "FT_a = torch.FloatTensor\n",
    "\n",
    "cuda = True if torch.cuda.is_available() else False\n",
    "\n",
    "if cuda:\n",
    "\tgenerator.cuda()\n",
    "\tdiscriminator.cuda()\n",
    "\ta_loss.cuda()\n",
    "\tFT = torch.cuda.LongTensor\n",
    "\tFT_a = torch.cuda.FloatTensor"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "\n",
    "# training\n",
    "for epoch in range(epochs):\n",
    "\tfor i, (imgs, labels) in enumerate(dataloader):\n",
    "\t\tbatch_size = imgs.shape[0]\n",
    "\n",
    "\t\t# convert img, labels into proper form\n",
    "\t\timgs = Variable(imgs.type(FT_a))\n",
    "\t\tlabels = Variable(labels.type(FT))\n",
    "\n",
    "\t\t# creating real and fake tensors of labels\n",
    "\t\treall = Variable(FT_a(batch_size,1).fill_(real_label))\n",
    "\t\tf_label = Variable(FT_a(batch_size,1).fill_(fake_label))\n",
    "\n",
    "\t\t# initializing gradient\n",
    "\t\tgen_optimizer.zero_grad()\n",
    "\t\td_optimizer.zero_grad()\n",
    "\n",
    "\t\t#### TRAINING GENERATOR ####\n",
    "\t\t# Feeding generator noise and labels\n",
    "\t\tnoise = Variable(FT_a(np.random.normal(0, 1,(batch_size, latentdim))))\n",
    "\t\tgen_labels = Variable(FT(np.random.randint(0, n_classes, batch_size)))\n",
    "\n",
    "\t\tgen_imgs = generator(noise, gen_labels)\n",
    "\n",
    "\t\t# Ability for discriminator to discern the real v generated images\n",
    "\t\tvalidity = discriminator(gen_imgs, gen_labels)\n",
    "\n",
    "\t\t# Generative loss function\n",
    "\t\tg_loss = a_loss(validity, reall)\n",
    "\n",
    "\t\t# Gradients\n",
    "\t\tg_loss.backward()\n",
    "\t\tgen_optimizer.step()\n",
    "\n",
    "\t\t#### TRAINING DISCRIMINTOR ####\n",
    "\n",
    "\t\td_optimizer.zero_grad()\n",
    "\n",
    "\t\t# Loss for real images and labels\n",
    "\t\tvalidity_real = discriminator(imgs, labels)\n",
    "\t\td_real_loss = a_loss(validity_real, reall)\n",
    "\n",
    "\t\t# Loss for fake images and labels\n",
    "\t\tvalidity_fake = discriminator(gen_imgs.detach(), gen_labels)\n",
    "\t\td_fake_loss = a_loss(validity_fake, f_label)\n",
    "\n",
    "\t\t# Total discriminator loss\n",
    "\t\td_loss = 0.5 * (d_fake_loss+d_real_loss)\n",
    "\n",
    "\t\t# calculates discriminator gradients\n",
    "\t\td_loss.backward()\n",
    "\t\td_optimizer.step()\n",
    "\n",
    "\n",
    "\t\tif i%100 == 0:\n",
    "\t\t\tvutils.save_image(gen_imgs, '%s/real_samples.png' % output, normalize=True)\n",
    "\t\t\tfake = generator(noise, gen_labels)\n",
    "\t\t\tvutils.save_image(fake.detach(), '%s/fake_samples_epoch_%03d.png' % (output, epoch), normalize=True)\n",
    "\n",
    "\tprint(\"[Epoch: %d/%d]\" \"[D loss: %f]\" \"[G loss: %f]\" % (epoch+1, epochs, d_loss.item(), g_loss.item()))\n",
    "\n",
    "\t# checkpoints\n",
    "\ttorch.save(generator.state_dict(), '%s/generator_epoch_%d.pth' % (output, epoch))\n",
    "\ttorch.save(discriminator.state_dict(), '%s/generator_epoch_%d.pth' % (output, epoch))\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}