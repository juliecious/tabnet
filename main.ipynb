{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torchvision.utils as vutils\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [],
   "source": [
    "from pytorch_tabnet.tab_network import TabNetEncoder, TabNetDecoder, EmbeddingGenerator\n",
    "from torch.distributions.normal import Normal\n",
    "\n",
    "class Generator(nn.Module):\n",
    "\t\"\"\"\n",
    "\tDefines generator architecture\n",
    "\n",
    "\tParameters\n",
    "\t----------\n",
    "\tinput_dim : int\n",
    "\t\tNumber of features\n",
    "\toutput_dim : int or list of int for multi task classification\n",
    "\t\tDimension of network output\n",
    "\t\texamples : one for regression, 2 for binary classification etc...\n",
    "\t\"\"\"\n",
    "\tdef __init__(self, input_dim, output_dim, cat_dims, cat_idxs, cat_emb_dim):\n",
    "\t\tsuper(Generator, self).__init__()\n",
    "\t\tself.embed = EmbeddingGenerator(input_dim, cat_dims, cat_idxs, cat_emb_dim)\n",
    "\t\tself.encode = TabNetEncoder(input_dim, output_dim)\n",
    "\t\tself.decode = TabNetDecoder(input_dim)\n",
    "\n",
    "\tdef forward(self, data):\n",
    "\t\t\"\"\"\n",
    "\t\tFunction for completing a forward pass of the Decoder: Given a noise vector,\n",
    "\t\treturns a generated data.\n",
    "\n",
    "\t\tParameters\n",
    "\t\t----------\n",
    "\t\t\tdata: Tensor\n",
    "\t\t\t\ttabular data tensor with dimensions (batch_size, im_chan, im_height, im_width)\n",
    "\t\tReturns\n",
    "        -------\n",
    "\t\tdecoding:\n",
    "\t\t\tthe autoencoded tabular data\n",
    "\t\tq_dist:\n",
    "\t\t\tthe z-distribution of the encoding\n",
    "\t\t\"\"\"\n",
    "\t\tembedded_x = self.embed(data)\n",
    "\t\tq_mean, q_stddev = self.encode(embedded_x)\n",
    "\t\tq_dist = Normal(q_mean, q_stddev)\n",
    "\t\tz_sample = q_dist.rsample() # Sample once from each distribution, using the `rsample` notation\n",
    "\t\tdecoding = self.decode(z_sample)\n",
    "\t\treturn decoding, q_dist\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(28*28, 300),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(300, 100),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(100, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)\n",
    "        out = self.model(x)\n",
    "        return out.view(x.size(0))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from torch.distributions.kl import kl_divergence\n",
    "\n",
    "def kl_divergence_loss(q_dist):\n",
    "    return kl_divergence(\n",
    "        q_dist, Normal(torch.zeros_like(q_dist.mean), torch.ones_like(q_dist.stddev))\n",
    "    ).sum(-1)\n",
    "\n",
    "reconstruction_loss = nn.BCELoss(reduction='sum')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from torch.utils.data.dataloader import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "train_dataloader = None\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "num_epochs = 5\n",
    "lr = 0.001\n",
    "discriminator = Discriminator()\n",
    "generator = Generator().to(device)\n",
    "d_optimizer = torch.optim.Adam(discriminator.parameters(), lr=lr)\n",
    "g_optimizer = torch.optim.Adam(generator.parameters(), lr=lr)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for epoch in range(num_epochs):\n",
    "\tfor data, _ in train_dataloader:\n",
    "\n",
    "\t\t# Train the Discriminator\n",
    "\n",
    "\t\tnoise = torch.randn((data.size(0), 100))\n",
    "\t\tfake_data = generator(noise)\n",
    "\n",
    "\t\tinputs = torch.cat([data, fake_data])\n",
    "\t\tlabels = torch.cat([torch.zeros(data.size(0)), # real\n",
    "\t\t\t\t\t\t\ttorch.ones(fake_data.size(0))]) # fake\n",
    "\n",
    "\t\td_outputs = discriminator(inputs)\n",
    "\t\td_loss = reconstruction_loss(d_outputs, labels)\n",
    "\t\td_loss.backward()\n",
    "\t\td_optimizer.step()\n",
    "\t\td_optimizer.zero_grad()\n",
    "\n",
    "\t\t# Train the Generator\n",
    "\t\tdata = data.to(device)\n",
    "\t\tg_optimizer.zero_grad()\n",
    "\t\tfake_data, encoding = generator(data)\n",
    "\t\toutputs = discriminator(fake_data)\n",
    "\n",
    "\t\tg_loss = reconstruction_loss(outputs, torch.zeros(data.size(0)))\n",
    "\t\tg_loss.backward()\n",
    "\t\tg_optimizer.step()\n",
    "\t\tg_optimizer.zero_grad()\n",
    "\n",
    "\tscores = torch.sigmoid(d_outputs)\n",
    "\treal_score = scores[:data.size(0)].data.mean()\n",
    "\tfake_score = scores[:data.size(0)].data.mean()\n",
    "\n",
    "\tprint(f'Epoch {epoch+1}/{num_epochs}, d_loss: {d_loss.item():.4f}, g_loss: {g_loss.item():.4f}, \\\n",
    "\t D(x): {real_score:.2f}, D(G(z)): {fake_score:.2f}' )\n",
    "\n",
    "\n",
    "\n",
    "\t# plot data\n",
    "\tnum_test_samples = 16\n",
    "\ttest_noise = torch.randn(num_test_samples, 100)\n",
    "\tgenerator.eval()\n",
    "\tdiscriminator.eval()\n",
    "\ttest_images = generator(test_noise)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}